{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RVbo8b69Gakwr3sTTOY492hB38XGlMWK",
      "authorship_tag": "ABX9TyOf07FZqRHexBLO4E/9gTcZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeYuuuan/MLP_EIT/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FFO9c0u8eL5J"
      },
      "outputs": [],
      "source": [
        "import scipy.io as scio\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import TensorDataset,DataLoader,Dataset\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gXZckw-7eSl5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "DNMPUHXieTSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomEITDataSet(Dataset):\n",
        "    \"\"\"EIT simulation dataset\"\"\"\n",
        "    def __init__(self, dataset_type):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dfir: Directory with all the data.\n",
        "        \"\"\"  \n",
        "\n",
        "        dic_path = \"/content/drive/MyDrive/Colab Notebooks/EIT/EITDeepLearning/data/\"\n",
        "        \n",
        "        # load the .csv simulation data \n",
        "        # voltage_data = np.loadtxt(dic_path + 'datacsv1.csv', delimiter=';', dtype=np.float32)\n",
        "        # label = np.loadtxt(dic_path + 'datacsv.csv', delimiter=';', dtype=np.float32)\n",
        "        print(f\"Loading {dataset_type}\")\n",
        "        print(\"Loading voltages...\")\n",
        "\n",
        "        \n",
        "        time_0 = time.time()\n",
        "        voltage_data = np.loadtxt(dic_path + 'data04.csv', delimiter=',', dtype=np.float32)\n",
        "        time_1 = time.time() - time_0\n",
        "        print(f\"using {time_1}s.\")\n",
        "        print(\"Loading labels...\")\n",
        "        label = np.loadtxt(dic_path + 'data04R.csv', delimiter=',', dtype=np.float32)\n",
        "        time_2 = time.time() - time_0\n",
        "        print(f\"Using {time_2}s.\")\n",
        "  \n",
        "        # split train/test dataset\n",
        "        print(\"Creating the dataset...\")\n",
        "        X_train_raw, X_test_raw, y_train, y_test = train_test_split(voltage_data, label, test_size=0.2, random_state=42)\n",
        "\n",
        "        X_train = np.zeros([X_train_raw.shape[0], 1, 16, 12]) # shape: count * 1 * 16(angle) * 12(voltage)\n",
        "        X_test = np.zeros([X_test_raw.shape[0], 1, 16, 12])\n",
        "\n",
        "        for i, data_element in enumerate(X_train_raw):\n",
        "            X_train[i, 0, :, :] = X_train_raw[i].reshape(16, 12)\n",
        "        for i, data_element in enumerate(X_test_raw):\n",
        "            X_test[i, 0, :, :] = X_test_raw[i].reshape(16,12)\n",
        "        \n",
        "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "        y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "        y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "        # return different type of data\n",
        "        if dataset_type == 'train':\n",
        "            self.x = X_train\n",
        "            self.y = y_train\n",
        "            self.len = X_train.shape[0]\n",
        "            time_3 = time.time() - time_0\n",
        "            print(f\"Finished loading train set! Using{time_3}s.\")\n",
        "        elif dataset_type == 'test':\n",
        "            self.x = X_test\n",
        "            self.y = y_test\n",
        "            self.len = X_test.shape[0]\n",
        "            time_4 = time.time() - time_0\n",
        "            print(f\"Finished loading test set! Using{time_4}s.\")\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "# Load  EIT dataset.\n",
        "batch_size = 64\n",
        "dataset_train = CustomEITDataSet('train')\n",
        "train_dataloader = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataset_test = CustomEITDataSet('test')  \n",
        "test_dataloader = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=True)\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCF9Zkd3eQu6",
        "outputId": "7aca30d0-93d5-4920-def3-b9303dcbe402"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train\n",
            "Loading voltages...\n",
            "using 2.11143159866333s.\n",
            "Loading labels...\n",
            "Using 8.320978164672852s.\n",
            "Creating the dataset...\n",
            "Finished loading train set! Using8.489453554153442s.\n",
            "Loading test\n",
            "Loading voltages...\n",
            "using 2.6979057788848877s.\n",
            "Loading labels...\n",
            "Using 7.732635974884033s.\n",
            "Creating the dataset...\n",
            "Finished loading test set! Using7.854297876358032s.\n",
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 16, 12])\n",
            "shape of y: torch.Size([64, 576]) torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP model"
      ],
      "metadata": {
        "id": "zEjiAHdweWAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(192, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 576),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = nn.functional.normalize(x)\n",
        "        x = self.linear_relu_stack(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LckMWFjmehp9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY6b-psYemcs",
        "outputId": "97202ffd-b191-4d3f-b0f0-471e4c490e02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=192, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=576, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train & test"
      ],
      "metadata": {
        "id": "fOgTydkRenHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        \n",
        "        # comput prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        #Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"training loss: {loss:>7f} [{current:5d}/{size:>5d}]\")\n",
        "    \n",
        "    return train_loss /num_batches\n",
        "\n"
      ],
      "metadata": {
        "id": "Abisl1InetBS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    print(f\"Test loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "    return test_loss\n"
      ],
      "metadata": {
        "id": "XbBoyRkrev3k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## training"
      ],
      "metadata": {
        "id": "DVJrWOU_eshO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l7q1-ArTe1Fa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "epoch = 40\n",
        "\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "for t in range(epoch):\n",
        "    print(f\"Epoch {t+1}--------------------------------\")\n",
        "    train_loss.append(train(train_dataloader, model, loss_fn, optimizer))\n",
        "    test_loss.append(test(test_dataloader, model, loss_fn))\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "lEYglSoscXdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85900fcd-6b9a-49b1-fb70-f93a365a3f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1--------------------------------\n",
            "training loss: 0.026406 [   64/20352]\n",
            "training loss: 0.002034 [ 6464/20352]\n",
            "training loss: 0.002038 [12864/20352]\n",
            "training loss: 0.002024 [19264/20352]\n",
            "Test loss: 0.002032 \n",
            "\n",
            "Epoch 2--------------------------------\n",
            "training loss: 0.002057 [   64/20352]\n",
            "training loss: 0.002063 [ 6464/20352]\n",
            "training loss: 0.002030 [12864/20352]\n",
            "training loss: 0.002017 [19264/20352]\n",
            "Test loss: 0.002028 \n",
            "\n",
            "Epoch 3--------------------------------\n",
            "training loss: 0.002054 [   64/20352]\n",
            "training loss: 0.002039 [ 6464/20352]\n",
            "training loss: 0.002037 [12864/20352]\n",
            "training loss: 0.002019 [19264/20352]\n",
            "Test loss: 0.002021 \n",
            "\n",
            "Epoch 4--------------------------------\n",
            "training loss: 0.002026 [   64/20352]\n",
            "training loss: 0.002021 [ 6464/20352]\n",
            "training loss: 0.001955 [12864/20352]\n",
            "training loss: 0.002005 [19264/20352]\n",
            "Test loss: 0.001998 \n",
            "\n",
            "Epoch 5--------------------------------\n",
            "training loss: 0.001984 [   64/20352]\n",
            "training loss: 0.001937 [ 6464/20352]\n",
            "training loss: 0.001959 [12864/20352]\n",
            "training loss: 0.001866 [19264/20352]\n",
            "Test loss: 0.001909 \n",
            "\n",
            "Epoch 6--------------------------------\n",
            "training loss: 0.001876 [   64/20352]\n",
            "training loss: 0.001885 [ 6464/20352]\n",
            "training loss: 0.001865 [12864/20352]\n",
            "training loss: 0.001848 [19264/20352]\n",
            "Test loss: 0.001838 \n",
            "\n",
            "Epoch 7--------------------------------\n",
            "training loss: 0.001867 [   64/20352]\n",
            "training loss: 0.001797 [ 6464/20352]\n",
            "training loss: 0.001752 [12864/20352]\n",
            "training loss: 0.001800 [19264/20352]\n",
            "Test loss: 0.001772 \n",
            "\n",
            "Epoch 8--------------------------------\n",
            "training loss: 0.001733 [   64/20352]\n",
            "training loss: 0.001783 [ 6464/20352]\n",
            "training loss: 0.001673 [12864/20352]\n",
            "training loss: 0.001693 [19264/20352]\n",
            "Test loss: 0.001704 \n",
            "\n",
            "Epoch 9--------------------------------\n",
            "training loss: 0.001697 [   64/20352]\n",
            "training loss: 0.001640 [ 6464/20352]\n",
            "training loss: 0.001667 [12864/20352]\n",
            "training loss: 0.001580 [19264/20352]\n",
            "Test loss: 0.001644 \n",
            "\n",
            "Epoch 10--------------------------------\n",
            "training loss: 0.001686 [   64/20352]\n",
            "training loss: 0.001643 [ 6464/20352]\n",
            "training loss: 0.001533 [12864/20352]\n",
            "training loss: 0.001619 [19264/20352]\n",
            "Test loss: 0.001583 \n",
            "\n",
            "Epoch 11--------------------------------\n",
            "training loss: 0.001591 [   64/20352]\n",
            "training loss: 0.001448 [ 6464/20352]\n",
            "training loss: 0.001548 [12864/20352]\n",
            "training loss: 0.001431 [19264/20352]\n",
            "Test loss: 0.001527 \n",
            "\n",
            "Epoch 12--------------------------------\n",
            "training loss: 0.001470 [   64/20352]\n",
            "training loss: 0.001475 [ 6464/20352]\n",
            "training loss: 0.001458 [12864/20352]\n",
            "training loss: 0.001412 [19264/20352]\n",
            "Test loss: 0.001480 \n",
            "\n",
            "Epoch 13--------------------------------\n",
            "training loss: 0.001417 [   64/20352]\n",
            "training loss: 0.001474 [ 6464/20352]\n",
            "training loss: 0.001379 [12864/20352]\n",
            "training loss: 0.001398 [19264/20352]\n",
            "Test loss: 0.001416 \n",
            "\n",
            "Epoch 14--------------------------------\n",
            "training loss: 0.001431 [   64/20352]\n",
            "training loss: 0.001416 [ 6464/20352]\n",
            "training loss: 0.001340 [12864/20352]\n",
            "training loss: 0.001439 [19264/20352]\n",
            "Test loss: 0.001362 \n",
            "\n",
            "Epoch 15--------------------------------\n",
            "training loss: 0.001436 [   64/20352]\n",
            "training loss: 0.001307 [ 6464/20352]\n",
            "training loss: 0.001328 [12864/20352]\n",
            "training loss: 0.001336 [19264/20352]\n",
            "Test loss: 0.001350 \n",
            "\n",
            "Epoch 16--------------------------------\n",
            "training loss: 0.001340 [   64/20352]\n",
            "training loss: 0.001380 [ 6464/20352]\n",
            "training loss: 0.001228 [12864/20352]\n",
            "training loss: 0.001376 [19264/20352]\n",
            "Test loss: 0.001323 \n",
            "\n",
            "Epoch 17--------------------------------\n",
            "training loss: 0.001337 [   64/20352]\n",
            "training loss: 0.001235 [ 6464/20352]\n",
            "training loss: 0.001272 [12864/20352]\n",
            "training loss: 0.001260 [19264/20352]\n",
            "Test loss: 0.001270 \n",
            "\n",
            "Epoch 18--------------------------------\n",
            "training loss: 0.001218 [   64/20352]\n",
            "training loss: 0.001273 [ 6464/20352]\n",
            "training loss: 0.001249 [12864/20352]\n",
            "training loss: 0.001298 [19264/20352]\n",
            "Test loss: 0.001239 \n",
            "\n",
            "Epoch 19--------------------------------\n",
            "training loss: 0.001194 [   64/20352]\n",
            "training loss: 0.001276 [ 6464/20352]\n",
            "training loss: 0.001172 [12864/20352]\n",
            "training loss: 0.001277 [19264/20352]\n",
            "Test loss: 0.001208 \n",
            "\n",
            "Epoch 20--------------------------------\n",
            "training loss: 0.001178 [   64/20352]\n",
            "training loss: 0.001255 [ 6464/20352]\n",
            "training loss: 0.001118 [12864/20352]\n",
            "training loss: 0.001217 [19264/20352]\n",
            "Test loss: 0.001202 \n",
            "\n",
            "Epoch 21--------------------------------\n",
            "training loss: 0.001239 [   64/20352]\n",
            "training loss: 0.001090 [ 6464/20352]\n",
            "training loss: 0.001200 [12864/20352]\n",
            "training loss: 0.001218 [19264/20352]\n",
            "Test loss: 0.001163 \n",
            "\n",
            "Epoch 22--------------------------------\n",
            "training loss: 0.001191 [   64/20352]\n",
            "training loss: 0.001056 [ 6464/20352]\n",
            "training loss: 0.001150 [12864/20352]\n",
            "training loss: 0.001101 [19264/20352]\n",
            "Test loss: 0.001136 \n",
            "\n",
            "Epoch 23--------------------------------\n",
            "training loss: 0.001068 [   64/20352]\n",
            "training loss: 0.001139 [ 6464/20352]\n",
            "training loss: 0.001088 [12864/20352]\n",
            "training loss: 0.001097 [19264/20352]\n",
            "Test loss: 0.001110 \n",
            "\n",
            "Epoch 24--------------------------------\n",
            "training loss: 0.001121 [   64/20352]\n",
            "training loss: 0.001091 [ 6464/20352]\n",
            "training loss: 0.001135 [12864/20352]\n",
            "training loss: 0.000972 [19264/20352]\n",
            "Test loss: 0.001113 \n",
            "\n",
            "Epoch 25--------------------------------\n",
            "training loss: 0.001118 [   64/20352]\n",
            "training loss: 0.001205 [ 6464/20352]\n",
            "training loss: 0.001130 [12864/20352]\n",
            "training loss: 0.001104 [19264/20352]\n",
            "Test loss: 0.001081 \n",
            "\n",
            "Epoch 26--------------------------------\n",
            "training loss: 0.001048 [   64/20352]\n",
            "training loss: 0.000987 [ 6464/20352]\n",
            "training loss: 0.001034 [12864/20352]\n",
            "training loss: 0.001108 [19264/20352]\n",
            "Test loss: 0.001080 \n",
            "\n",
            "Epoch 27--------------------------------\n",
            "training loss: 0.001058 [   64/20352]\n",
            "training loss: 0.001059 [ 6464/20352]\n",
            "training loss: 0.001077 [12864/20352]\n",
            "training loss: 0.001075 [19264/20352]\n",
            "Test loss: 0.001048 \n",
            "\n",
            "Epoch 28--------------------------------\n",
            "training loss: 0.001012 [   64/20352]\n",
            "training loss: 0.001096 [ 6464/20352]\n",
            "training loss: 0.001077 [12864/20352]\n",
            "training loss: 0.000983 [19264/20352]\n",
            "Test loss: 0.001043 \n",
            "\n",
            "Epoch 29--------------------------------\n",
            "training loss: 0.000896 [   64/20352]\n",
            "training loss: 0.001041 [ 6464/20352]\n",
            "training loss: 0.001003 [12864/20352]\n",
            "training loss: 0.001095 [19264/20352]\n",
            "Test loss: 0.001037 \n",
            "\n",
            "Epoch 30--------------------------------\n",
            "training loss: 0.001058 [   64/20352]\n",
            "training loss: 0.000979 [ 6464/20352]\n",
            "training loss: 0.000993 [12864/20352]\n",
            "training loss: 0.000960 [19264/20352]\n",
            "Test loss: 0.001006 \n",
            "\n",
            "Epoch 31--------------------------------\n",
            "training loss: 0.000961 [   64/20352]\n",
            "training loss: 0.001035 [ 6464/20352]\n",
            "training loss: 0.001065 [12864/20352]\n",
            "training loss: 0.000981 [19264/20352]\n",
            "Test loss: 0.000990 \n",
            "\n",
            "Epoch 32--------------------------------\n",
            "training loss: 0.000933 [   64/20352]\n",
            "training loss: 0.000984 [ 6464/20352]\n",
            "training loss: 0.001021 [12864/20352]\n",
            "training loss: 0.000945 [19264/20352]\n",
            "Test loss: 0.000996 \n",
            "\n",
            "Epoch 33--------------------------------\n",
            "training loss: 0.001012 [   64/20352]\n",
            "training loss: 0.000991 [ 6464/20352]\n",
            "training loss: 0.000908 [12864/20352]\n",
            "training loss: 0.000916 [19264/20352]\n",
            "Test loss: 0.000963 \n",
            "\n",
            "Epoch 34--------------------------------\n",
            "training loss: 0.000829 [   64/20352]\n",
            "training loss: 0.000917 [ 6464/20352]\n",
            "training loss: 0.000952 [12864/20352]\n",
            "training loss: 0.000859 [19264/20352]\n",
            "Test loss: 0.000960 \n",
            "\n",
            "Epoch 35--------------------------------\n",
            "training loss: 0.001004 [   64/20352]\n",
            "training loss: 0.000916 [ 6464/20352]\n",
            "training loss: 0.000892 [12864/20352]\n",
            "training loss: 0.000939 [19264/20352]\n"
          ]
        }
      ]
    }
  ]
}